## EDA

Самые важные результаты из предварительного анализа были следующие:
- удаление неинформативных признаков
    - `capacity_bytes`, `model`, `serial_number`, `failure` - неинформативность
    - `smart_198_raw` - тоже самое, что и `smart_197_raw`
- отсутствие `nan` значений
- сильный дисбаланс классов
#### Реализация
Реализация в `eda.py`. Файл содержит в себе построение различных графиков, временных рядов для определённых дисков (т.е. дисков с определёнными серийными номерами), построение гистограмм, ящиков с усами и т.д.
## Обработка данных
### Дисбаланс классов
Для решения проблемы дисбаланса классов были указаны веса классов для моделей (градиентного бустинга), а также SMOTE генерация синтетических данных различных видов, Adasyn и методы undersampling-а. 
**Самое важное, что стоит отметить**, это то, что из-за сильного дисбаланса методы генерации синтетических данных **сильно понижают** значение метрики **precision**, но немного повышают recall.
### Нормализация
Были протестированы следующие методы нормализации данных:
- логарифмизация
- преобразования Бокса-Кокса
- смешанные преобразования логарифмизации и Бокса-Кокса (выборочно для различных столбцов)
- **преобразования Йео-Джонсона**
- нормализация по формулам из [документации seagate](https://t1.daumcdn.net/brunch/service/user/axm/file/zRYOdwPu3OMoKYmBOby1fEEQEbU.pdf#page=7.10) (`smart_n_normalized`)

### Получение новых признаков
По причине наличия временной составляющей в задаче, было принято решение добавить следующие признаки:
- сдвинутые признаки (значение того же самого признака, но за предыдущий день) (`shift_smart_n_raw_n)
- разность признаков (разность между текущим значением признака и сдвинутым) (`diff_smart_n_raw_n`)
- нормированные по формулам из [документации seagate](https://t1.daumcdn.net/brunch/service/user/axm/file/zRYOdwPu3OMoKYmBOby1fEEQEbU.pdf#page=7.10) признаки (`smart_n_normalized`)
#### Реализация
Реализация в `preprocessing.py`. Представленные в этом файле классы реализуют различные методы нормализации, разбиение датасета, а также метод для добавления целевой переменной, поскольку исходно она отсутствует в датасете; преобразование типов данных для уменьшения затрата памяти; удаление фичей, имеющих низкую важность. Изменяя порядок применения операций, изменяем результат - если сначала произвести разбиение, а затем нормализацию, то нормализация будет произведена для каждой получившейся части отдельно

По итогам преобразований чаще всего были использованы Йео-Джонсона, поскольку они поддерживают отрицательные значения, которые могли быть получены в результате создания сдвинутых признаков и разностных; а также часто использовались преобразования по формулам нормализации seagate. 

![[./images/Pasted image 20241223002553.png]]
*feature importance*


## Методы решения
### Модели градиентного бустинга
Были протестированы разные модели градиентного бустинга: XGBoost, CatBoost, LightGBM.
Для этих классов была реализована оптимизация гиперпараметров различными методами: GridSearch, RandomSearch, TPE оптимизация.
Реализация в `MyModel.py`

### Ансамбль
Был протестирован ансамбль методов градиентного бустинга: XGBoost, CatBoost, LightGBM. Предикт алгоритма получается как взвешенная сумма трёх моделей.  
Реализация в `XCL.py`. Есть возможность оптимизации гиперпараметров трёх моделей, а также оптимизации весов моделей.
##### Реализация
Реализация в `XCL.py`. 
### Двухслойный XGBoost
Основная идея заключается в том, что сначала происходит тюнинг первой модели XGBoost, запоминая важные признаки. Далее, она на stratified K fold обучается с нуля (но с оптимизированными параметрами) и делает предикты на валидационных выборках, которые мы запоминаем (похоже на стекинг). Вторая модель обучается на важных признаках исходного датасета + предиктах первой модели. Т.е. итоговый предикт выносит модель второго слоя, используя в качестве датасета объединение важных признаков исходного датасета и предиктов модели первого слоя.
##### Реализация
Реализация в `DoubleLayer.py`. До этого в качестве основной модели использовался LightGBM по причине быстрой работы. Есть возможность использовать готовые настройки модели.
### Блендинг
##### Реализация
Реализация в `Blending.py`. В качестве моделей тестировались MLP, наивный байесовский классификатор, CatBoost, XGBoost, случайный лес, логистическая регрессия. По итогу комбинация, продемонстрировавшая лучше результаты: случайный лес, XGBoost, CatBoost, и CatBoost или логистическая регрессия  в качестве мета-модели. CatBoost был выбран по причине того, что он показывает хорошие результаты "из коробки", т.е. без оптимизации гиперпараметров, случайный лес и XGBoost использовались с оптимизированными параметрами.
### Стекинг
##### Реализация
Реализация в `StackingSK.py`. Модели выбраны всё те же, что и в блендинге. Лучший набор: случайный лес, CatBoost, XGBoost и в качесте мета модели: логистическая регрессия
## Логирование
После выполнение каждой итерации обучения и тестирования (препроцессинг, обучение, оптимизация параметров, тестирование) исчерпывающая информация о итерации, а именно
* Используемые модели (для алгоритмов с использованием нескольких моделей)
* Операции препроцессора
* Операции оптимизации
* Метрики
сохраняется в .json файл в папку saved. Также сохраняется и дамп модели (моделей), а также преобразованных датасетов 
*В репозитории хранятся логи не всех тестов, мы оставили только самые наглядные. Также не загружали дампы моделей и датасетов*